defaults:
  - _self_
  - override hydra/launcher: submitit_slurm

ckpt_base_path: ./
data_path: /scratch/qt2094/DL24FA

hydra:
  run:
    dir: ${ckpt_base_path}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${ckpt_base_path}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  launcher:
    submitit_folder: ${hydra.sweep.dir}/.submitit/%j
    nodes: 1
    tasks_per_node: 1
    cpus_per_task: 1
    mem_gb: 256
    gres: "gpu:a100:1"
    # gres: "gpu:volta:4"
    # constraint: "volta32gb"
    timeout_min: 4320
    setup: ["export DEBUGVAR=$(scontrol show hostnames $SLURM_JOB_NODELIST)",
            "export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)",
            "export MASTER_PORT=$(for port in $(shuf -i 30000-65500 -n 20); do if [[ $(netstat -tupln 2>&1 | grep $port | wc -l) -eq 0 ]] ; then echo $port; break; fi; done;)",]

training:
  epochs: 100
  warmup_epochs: 5
  start_lr: 0.0002                   # Starting learning rate during warmup
  ref_lr: 0.001                      # Reference learning rate after warmup
  final_lr: 1.0e-06                  # Final learning rate at the end of training

  weight_decay: 0.04                 # Initial weight decay
  final_weight_decay: 0.4            # Final weight decay

  use_bfloat16: true                 # Use bfloat16 for mixed precision training
  ipe_scale: 1.0                     # Scaling factor for iterations per epoch
  ema:
    - 0.996
    - 1.0

model:
  action_dim: 2