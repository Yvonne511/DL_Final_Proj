{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_850272/24337169.py:167: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\".\", config_name=\"config\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dataset import create_wall_dataloader\n",
    "from evaluator import ProbingEvaluator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models import MockModel\n",
    "import glob\n",
    "from utils.model import JEPA_Model, init_opt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Check for GPU availability.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    return device\n",
    "\n",
    "\n",
    "def load_data(device, cfg):\n",
    "    data_path = \"/scratch/qt2094/DL24FA\"\n",
    "    train_ds = create_wall_dataloader(\n",
    "        data_path=f\"{data_path}/train\",\n",
    "        probing=False,\n",
    "        device=device,\n",
    "        train=True,\n",
    "        batch_size=cfg.training.batch_size\n",
    "    )\n",
    "    probe_train_ds = create_wall_dataloader(\n",
    "        data_path=f\"{data_path}/probe_normal/train\",\n",
    "        probing=True,\n",
    "        device=device,\n",
    "        train=True,\n",
    "    )\n",
    "\n",
    "    probe_val_normal_ds = create_wall_dataloader(\n",
    "        data_path=f\"{data_path}/probe_normal/val\",\n",
    "        probing=True,\n",
    "        device=device,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    probe_val_wall_ds = create_wall_dataloader(\n",
    "        data_path=f\"{data_path}/probe_wall/val\",\n",
    "        probing=True,\n",
    "        device=device,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    probe_val_ds = {\"normal\": probe_val_normal_ds, \"wall\": probe_val_wall_ds}\n",
    "\n",
    "    return train_ds, probe_train_ds, probe_val_ds\n",
    "\n",
    "\n",
    "def load_model(device, action_dim):\n",
    "    \"\"\"Load or initialize the model.\"\"\"\n",
    "    # TODO: Replace MockModel with your trained model\n",
    "    model = MockModel()\n",
    "    # model = JEPA_Model(device=device, action_dim=action_dim)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(device, model, probe_train_ds, probe_val_ds):\n",
    "    evaluator = ProbingEvaluator(\n",
    "        device=device,\n",
    "        model=model,\n",
    "        probe_train_ds=probe_train_ds,\n",
    "        probe_val_ds=probe_val_ds,\n",
    "        quick_debug=False,\n",
    "    )\n",
    "\n",
    "    prober = evaluator.train_pred_prober()\n",
    "\n",
    "    avg_losses = evaluator.evaluate_all(prober=prober)\n",
    "\n",
    "    for probe_attr, loss in avg_losses.items():\n",
    "        print(f\"{probe_attr} loss: {loss}\")\n",
    "    return avg_losses\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     device = get_device()\n",
    "#     probe_train_ds, probe_val_ds = load_data(device)\n",
    "#     model = load_model()\n",
    "#     evaluate_model(device, model, probe_train_ds, probe_val_ds)\n",
    "\n",
    "import os\n",
    "import hydra\n",
    "import wandb\n",
    "import submitit_patch\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "\n",
    "def save_continuous_frames_with_metadata(batch, output_dir=\"sample_frames\"):\n",
    "    \"\"\"\n",
    "    Save continuous frames from one sample and print corresponding locations and actions.\n",
    "\n",
    "    Args:\n",
    "        batch: WallSample with states, locations, and actions.\n",
    "        output_dir: Directory to save the frames.\n",
    "    \"\"\"\n",
    "    # Extract the first sample from the batch\n",
    "    states = batch.states[0]  # Shape: [channels, frames, height, width]\n",
    "    locations = batch.locations[0]  # Shape: [frames, 2]\n",
    "    actions = batch.actions[0]  # Shape: [frames - 1, 2]\n",
    "    frames, channels, height, width = states.shape\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save frames\n",
    "    for f in range(frames):\n",
    "        # Select the first channel for visualization\n",
    "        frame = states[f, 1, :, :].squeeze().cpu().numpy()  # Shape: [65, 65]\n",
    "\n",
    "        plt.imshow(frame, cmap=\"gray\")\n",
    "        plt.title(f\"Sample 0, Frame {f}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.savefig(f\"{output_dir}/frame_{f}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"Saved frames to {output_dir}\")\n",
    "\n",
    "    # Print corresponding locations and actions\n",
    "    print(\"Locations:\")\n",
    "    print(locations.cpu().numpy())\n",
    "\n",
    "    print(\"\\nActions:\")\n",
    "    print(actions.cpu().numpy())\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, cfg, start_time):\n",
    "    # Use the Hydra-generated directory structure\n",
    "    checkpoint_dir = f\"{cfg.ckpt_base_path}/{start_time}\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "    \n",
    "    # Save the current checkpoint\n",
    "    save_path = os.path.join(checkpoint_dir, f\"ckpt_{epoch}_{loss:.6f}.pth\")\n",
    "    torch.save(checkpoint, save_path)\n",
    "    \n",
    "    print(f\"Checkpoint saved at {save_path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, save_path=\"checkpoint.pth\"):\n",
    "    if os.path.isfile(save_path):\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        loss = checkpoint[\"loss\"]\n",
    "        print(f\"Checkpoint loaded from {save_path} at epoch {epoch}\")\n",
    "        return epoch, loss\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {save_path}\")\n",
    "        return 0, None\n",
    "    \n",
    "    \n",
    "@hydra.main(config_path=\".\", config_name=\"config\")\n",
    "def sweep_main(cfg: OmegaConf):\n",
    "    start_time = time.strftime(\"%Y-%m-%d/%H-%M-%S\")\n",
    "    sweep_cfg = OmegaConf.load(\"./sweep_config.yaml\")\n",
    "    with open_dict(cfg):\n",
    "        cfg[\"saved_folder\"] = os.getcwd()\n",
    "        print(f\"Saving everything in: {cfg['saved_folder']}\")\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"dl-final \",\n",
    "        config=OmegaConf.to_container(cfg),\n",
    "    )\n",
    "    device = get_device()\n",
    "    train_ds, probe_train_ds, probe_val_ds = load_data(device, cfg)\n",
    "    print(f\"Number of training batches: {len(train_ds)}\")\n",
    "    print(f\"Number of total batches: {len(train_ds) * cfg.training.epochs}\")\n",
    "\n",
    "    model_config = cfg.model\n",
    "    action_dim = model_config.action_dim\n",
    "\n",
    "    training_config = cfg.training\n",
    "    num_epochs = training_config.epochs\n",
    "    ipe = len(train_ds)\n",
    "    ema = training_config.ema\n",
    "\n",
    "    momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*training_config.ipe_scale)\n",
    "                          for i in range(int(ipe*num_epochs*training_config.ipe_scale)+1))\n",
    "                          \n",
    "    model = JEPA_Model(model_name=sweep_cfg.model.model_name, device=device, action_dim=action_dim, momentum_scheduler=momentum_scheduler)\n",
    "\n",
    "    optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
    "            model.observation_encoder,\n",
    "            model.predictor,\n",
    "            iterations_per_epoch=ipe,\n",
    "            start_lr=training_config.start_lr,\n",
    "            ref_lr=training_config.ref_lr,\n",
    "            warmup=training_config.warmup_epochs,\n",
    "            num_epochs=num_epochs,\n",
    "            wd=training_config.weight_decay,\n",
    "            final_wd=training_config.final_weight_decay,\n",
    "            final_lr=training_config.final_lr,\n",
    "            use_bfloat16=training_config.use_bfloat16,\n",
    "            ipe_scale=training_config.ipe_scale\n",
    "        )\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Linear):  # Adjust to target specific layers\n",
    "            nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "    # checkpoint_path = \"model_checkpoint.pth\"\n",
    "    # start_epoch, _ = load_checkpoint(model, optimizer, checkpoint_path)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_epoch = epoch + 1\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_f1_loss = 0.0\n",
    "        total_var_loss = 0.0\n",
    "        total_cov_loss = 0.0\n",
    "        len_train_ds = len(train_ds)\n",
    "        pbar = tqdm(train_ds, desc=f\"Training Epoch {epoch+1}\", leave=False)\n",
    "        \n",
    "        best_batch_loss = float('inf')\n",
    "       \n",
    "        total_batch = 0\n",
    "        for i, batch in enumerate(pbar):\n",
    "            total_batch += 1\n",
    "            \n",
    "            # print(f\"Batch states shape: {batch.states.shape}\")  # [64, 17, 2, 65, 65]\n",
    "            # print(f\"Batch locations shape: {batch.locations.shape}\") # [64, 17, 2]\n",
    "            # print(f\"Batch actions shape: {batch.actions.shape}\") # [64, 16, 2]\n",
    "            # save_continuous_frames_with_metadata(batch)\n",
    "            obs = batch.states # [64, 17, 2, 65, 65]\n",
    "            acts = batch.actions # [64, 17, 2]\n",
    "            optimizer.zero_grad()\n",
    "            loss, f1_loss, var_loss, cov_loss = model(obs, acts)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "            wd_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_f1_loss += f1_loss.item()\n",
    "            total_var_loss += var_loss.item()\n",
    "            total_cov_loss += cov_loss.item()\n",
    "\n",
    "\n",
    "            if total_batch % 2 == 0:\n",
    "                \n",
    "                saving_begin = time.time()\n",
    "                best_batch_loss = loss\n",
    "                save_checkpoint(model, optimizer, epoch + 1, best_batch_loss, cfg, start_time)\n",
    "                saving_end = time.time()\n",
    "                saving_time = saving_end - saving_begin\n",
    "                print(f\"Saving checkpoint took {saving_time:.2f} seconds\")\n",
    "\n",
    "            pbar.set_description(f\"Epoch {current_epoch} - Loss: {loss:.4f}, \"\n",
    "                                  f\"F1: {f1_loss:.4f}, \"\n",
    "                                  f\"Var: {var_loss:.4f}, \"\n",
    "                                    f\"Cov: {cov_loss:.4f}\")\n",
    "            wandb.log({\n",
    "                \"Batch Train Loss\": loss,\n",
    "                \"Batch F1 Loss\": f1_loss,\n",
    "                \"Batch Var Loss\": var_loss,\n",
    "                \"Batch Cov Loss\": cov_loss,\n",
    "                \"Epoch\": current_epoch,\n",
    "                \"Batch\": i+1\n",
    "            })\n",
    "\n",
    "        avg_loss = total_loss / len_train_ds\n",
    "        avg_f1_loss = total_f1_loss / len_train_ds\n",
    "        avg_var_loss = total_var_loss / len_train_ds\n",
    "        avg_cov_loss = total_cov_loss / len_train_ds\n",
    "        print(f\"Epoch [{current_epoch}/{num_epochs}] - Loss: {avg_loss:.4f}, F1 Loss: {avg_f1_loss:.4f}, \"\n",
    "          f\"Var Loss: {avg_var_loss:.4f}, Cov Loss: {avg_cov_loss:.4f}\")\n",
    "        wandb.log({\n",
    "            \"Epoch Summary\": {\n",
    "                \"Train Loss\": avg_loss,\n",
    "                \"F1 Loss\": avg_f1_loss,\n",
    "                \"Var Loss\": avg_var_loss,\n",
    "                \"Cov Loss\": avg_cov_loss,\n",
    "                \"Epoch\": current_epoch\n",
    "            }\n",
    "        })\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            save_checkpoint(model, optimizer, current_epoch, best_loss, cfg, start_time)\n",
    "\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. training.ref_lr uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 2. training.start_lr uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 3. training.weight_decay uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 4. training.final_lr uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: v1qft3a4\n",
      "Sweep URL: https://wandb.ai/qt2094-new-york-university/dl-final-sweep/sweeps/v1qft3a4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sweep_cfg = OmegaConf.load(\"./sweep_config.yaml\")\n",
    "sweep_dict = OmegaConf.to_container(sweep_cfg, resolve=True)\n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep_dict,project=\"dl-final-sweep\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the sweep with 5 total runs\n",
    "wandb.agent(sweep_id, function=sweep_main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
